import json
import os
import warnings
from typing import List, Dict, Optional
import argparse

import faiss
import torch
import numpy as np
from transformers import AutoConfig, AutoTokenizer, AutoModel
from tqdm import tqdm
import datasets

# ------------------- åŸºç¡€å·¥å…·å‡½æ•° -------------------
def load_corpus(corpus_path: str):
    corpus = datasets.load_dataset(
        'json', 
        data_files=corpus_path,
        split="train",
        num_proc=4
    )
    return corpus

def load_docs(corpus, doc_idxs):
    return [corpus[int(idx)] for idx in doc_idxs]

def load_model(model_path: str, use_fp16: bool = False):
    model_config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)
    model = AutoModel.from_pretrained(model_path, trust_remote_code=True)
    model.eval().cuda()
    return model.half() if use_fp16 else model, AutoTokenizer.from_pretrained(model_path, use_fast=True)

def pooling(pooler_output, last_hidden_state, attention_mask, method="mean"):
    if method == "mean":
        last_hidden = last_hidden_state.masked_fill(~attention_mask[..., None].bool(), 0.0)
        return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]
    elif method == "cls":
        return last_hidden_state[:, 0]
    elif method == "pooler":
        return pooler_output
    else:
        raise ValueError(f"Unsupported pooling method: {method}")

# ------------------- ç¼–ç å™¨ç±» -------------------
class Encoder:
    def __init__(self, model_name, model_path, pooling_method, max_length=256, use_fp16=False):
        self.model, self.tokenizer = load_model(model_path, use_fp16)
        self.model_name = model_name
        self.pooling_method = pooling_method
        self.max_length = max_length

    @torch.no_grad()
    def encode(self, texts: List[str], is_query=True) -> np.ndarray:
        if "e5" in self.model_name.lower():
            texts = [f"query: {t}" if is_query else f"passage: {t}" for t in texts]
        elif "bge" in self.model_name.lower() and is_query:
            texts = [f"Represent this sentence for searching relevant passages: {t}" for t in texts]

        inputs = self.tokenizer(texts, 
                               max_length=self.max_length,
                               padding=True,
                               truncation=True,
                               return_tensors="pt").to("cuda")

        if "T5" in type(self.model).__name__:
            decoder_input_ids = torch.zeros((inputs['input_ids'].shape[0], 1), dtype=torch.long).cuda()
            output = self.model(**inputs, decoder_input_ids=decoder_input_ids)
            emb = output.last_hidden_state[:, 0, :]
        else:
            output = self.model(**inputs)
            emb = pooling(output.pooler_output, 
                         output.last_hidden_state,
                         inputs['attention_mask'],
                         self.pooling_method)
            if "dpr" not in self.model_name.lower():
                emb = torch.nn.functional.normalize(emb, dim=-1)

        return emb.detach().cpu().numpy().astype(np.float32)

# ------------------- æ£€ç´¢å™¨åŸºç±» -------------------
class BaseRetriever:
    def __init__(self, config):
        self.config = config
        self.topk = config.retrieval_topk
        self.corpus = load_corpus(config.corpus_path)

    def batch_search(self, queries: List[str], num: int = None) -> List[List[Dict]]:
        raise NotImplementedError

class DenseRetriever(BaseRetriever):
    def __init__(self, config):
        super().__init__(config)
        self.index = faiss.read_index(config.index_path)
        if config.faiss_gpu:
            self.index = faiss.index_cpu_to_all_gpus(self.index)
        self.encoder = Encoder(
            model_name=config.retrieval_method,
            model_path=config.retrieval_model_path,
            pooling_method=config.retrieval_pooling_method,
            use_fp16=config.retrieval_use_fp16
        )
        self.batch_size = config.retrieval_batch_size

    def batch_search(self, queries: List[str], num: int = None) -> List[List[Dict]]:
        num = num or self.topk
        all_results = []
        
        for i in tqdm(range(0, len(queries), self.batch_size), desc="Searching"):
            batch = queries[i:i+self.batch_size]
            embs = self.encoder.encode(batch)
            scores, indices = self.index.search(embs, num)
            
            batch_docs = []
            for idxs in indices:
                batch_docs.append(load_docs(self.corpus, idxs))
            
            all_results.extend(batch_docs)
        return all_results

# ------------------- ä¸»ç¨‹åº -------------------
def main():
    parser = argparse.ArgumentParser(description="æ–‡æ¡£æ£€ç´¢ç³»ç»Ÿ")
    parser.add_argument("--index", type=str, required=True, help="FAISSç´¢å¼•è·¯å¾„")
    parser.add_argument("--corpus", type=str, required=True, help="è¯­æ–™åº“JSONLè·¯å¾„")
    parser.add_argument("--queries", type=str, required=True, help="æŸ¥è¯¢JSONLæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output", type=str, required=True, help="è¾“å‡ºJSONè·¯å¾„")
    parser.add_argument("--model", type=str, default="BAAI/bge-base-en-v1.5", help="æ£€ç´¢æ¨¡å‹åç§°æˆ–è·¯å¾„")
    parser.add_argument("--topk", type=int, default=5, help="æ£€ç´¢æ–‡æ¡£æ•°é‡")
    parser.add_argument("--batch_size", type=int, default=32, help="æ‰¹å¤„ç†å¤§å°")
    parser.add_argument("--gpu", action="store_true", help="ä½¿ç”¨GPUåŠ é€Ÿ")

    args = parser.parse_args()

    # é…ç½®å‚æ•°
    class Config:
        def __init__(self):
            self.retrieval_method = "dense"
            self.retrieval_topk = args.topk
            self.index_path = args.index
            self.corpus_path = args.corpus
            self.faiss_gpu = args.gpu
            self.retrieval_model_path = args.model
            self.retrieval_pooling_method = "cls"
            self.retrieval_use_fp16 = True
            self.retrieval_batch_size = args.batch_size

    # åˆå§‹åŒ–æ£€ç´¢å™¨
    retriever = DenseRetriever(Config())

    # åŠ è½½æŸ¥è¯¢æ•°æ®
    print("ğŸ” åŠ è½½æŸ¥è¯¢æ•°æ®...")
    with open(args.queries) as f:
        dataset = [json.loads(line) for line in f]

    queries = [item["question"] for item in dataset]

    # æ‰§è¡Œæ£€ç´¢
    print("ğŸš€ å¼€å§‹æ£€ç´¢...")
    results = retriever.batch_search(queries)

    # æ„å»ºè¾“å‡ºæ ¼å¼
    print("ğŸ“¦ æ ¼å¼åŒ–è¾“å‡º...")
    output_data = []
    for orig_item, docs in zip(dataset, results):
        context = []
        for doc in docs:
            # è§£ææ–‡æ¡£å†…å®¹
            title = doc.get("title", "æœªçŸ¥æ ‡é¢˜")
            content = doc.get("text", doc.get("contents", ""))
            
            # åˆ†å‰²æ®µè½ï¼ˆå‡è®¾åŸå§‹æ–‡æœ¬ç”¨æ¢è¡Œç¬¦åˆ†éš”ï¼‰
            paragraphs = [p.strip() for p in content.split("\n") if p.strip()]
            
            # å¦‚æœæ²¡æœ‰æœ‰æ•ˆæ®µè½ï¼Œä½¿ç”¨æ•´ä¸ªå†…å®¹ä½œä¸ºæ®µè½
            if not paragraphs:
                paragraphs = [content.strip()] if content.strip() else []
            
            context.append([title, paragraphs])
        
        output_item = {
            "question": orig_item["question"],
            "answer": orig_item["answer"],
            "search": context,
        }
        output_data.append(output_item)

    # ä¿å­˜ç»“æœ
    print("ğŸ’¾ ä¿å­˜ç»“æœåˆ°", args.output)
    with open(args.output, "w", encoding="utf-8") as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)

if __name__ == "__main__":
    main()

    #è¾“å…¥æ ¼å¼
#     {
#   "question": "é—®é¢˜æ–‡æœ¬",
#   "answer": "ç­”æ¡ˆæ–‡æœ¬",
#   "id": "å”¯ä¸€æ ‡è¯†ç¬¦ï¼ˆå¯é€‰ï¼‰",
#   "supporting_facts": ["ç›¸å…³äº‹å®åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰"]
# }
# å‘½ä»¤ç¤ºä¾‹
# python retrieval_system.py \
#   --index path/to/faiss.index \
#   --corpus path/to/corpus.jsonl \
#   --queries path/to/questions.jsonl \
#   --output path/to/output.json \
#   --model BAAI/bge-base-zh-v1.5 \
#   --topk 5 \
#   --batch_size 64 \
#   --gpu

# DATA_NAME=hotpot

# DATASET_PATH="/workspace/work/exp/grpo_kl/data/$DATA_NAME/hotpot_train_v1.1.json"

# TOPK=3

# INDEX_PATH=workspace/work/exp/grpo_kl/wiki/wiki-18
# CORPUS_PATH=workspace/work/exp/grpo_kl/wiki/wiki-18.jsonl
# SAVE_NAME=e5_${TOPK}_wiki18.json



# CUDA_VISIBLE_DEVICES=1,2,3,4,5,6,7 python retrieval.py --retrieval_method e5 \
#                     --topk $TOPK \
#                     --index $INDEX_PATH \
#                     --corpus $CORPUS_PATH \
#                     --queries $DATASET_PATH \
#                     --output $SAVE_NAME \
#                     --model "wiki" \
#                     --batch_size 64 \
#                     --gpu
#è¾“å‡ºæ–‡ä»¶ç¤ºä¾‹
# {
#   "question": "é—®é¢˜æ–‡æœ¬",
#   "answer": "ç­”æ¡ˆæ–‡æœ¬",
#   "context": [
#     ["æ–‡æ¡£æ ‡é¢˜1", ["æ®µè½1", "æ®µè½2", ...]],
#     ["æ–‡æ¡£æ ‡é¢˜2", ["æ®µè½1", ...]],
#     // ...æœ€å¤štopkä¸ªæ–‡æ¡£
#   ],
#   "_id": "åŸå§‹ID",
#   "supporting_facts": ["ç›¸å…³äº‹å®"]
# }